{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Workshop4_StatisticalInference.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGvLFU9dgAWe",
        "colab_type": "text"
      },
      "source": [
        "# Brief Primer on Descriptive Statistics\n",
        "## Athlete Statistics at the 2016 Rio Olympics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ulA8kx4gAWi",
        "colab_type": "text"
      },
      "source": [
        "This notebook serves as a very basic primer on Desrciptive Statistics and will explain concepts which are fundamental to understanding Inferential Statistics, its tools and techniques. More specifically, we will be looking at athlete data from the 2016 Rio Olympics and experimenting with the various theories of statistics explained in the slides.\n",
        "\n",
        "The dataset was obtained from Kaggle (https://www.kaggle.com/rio2016/olympic-games#_=_) and was uploaded by Rio 2016."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO-SzVFmgAWk",
        "colab_type": "text"
      },
      "source": [
        "## Importing Libraries and loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUJlcIiTgAWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMWJkjnagAWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi1-FWzFgAW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('athletes.csv')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvFTVLD6gAW-",
        "colab_type": "text"
      },
      "source": [
        "## Central Tendencies\n",
        "\n",
        "The central tendencies are values which represent the central or 'typical' value of the given distribution. The three most popular central tendency estimates are the mean, median and mode. Typically, in most cases, we resort to using mean (for normal distributions) and median (for skewed distributions) to report central tendency values.\n",
        "\n",
        "A good rule of thumb is to use mean when outliers don't affect its value and median when it does (Bill Gates joke, anyone?).\n",
        "\n",
        "Calculating the mean and median are extremely trivial with Pandas. In the following cell, we have calculated the mean and median of the athlete heights in the 2016 Olympics. As we can see below, the mean and the median are almost equal. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUr1iBLIgAXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "heights = df['height']\n",
        "heights.mean(), heights.median()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBcCigHmgAXI",
        "colab_type": "text"
      },
      "source": [
        "## Measures of Spread\n",
        "Apart from the central or typical value of the data, we are also interested in knowing how much the data spreads. That is, how far from the mean do values tend to go. Statistics equips us with two measures to quantitatively represent the spread: the variance and the standard deviation. They are dependent quantities, with the standard deviation being defined as the square root of variance.\n",
        "\n",
        "As above, calculating the standard deviation (and variance) is trivial with Pandas as can be seen below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5R94vjzgAXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "heights.std(), heights.var()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKfGo35GgAXV",
        "colab_type": "text"
      },
      "source": [
        "The mean and the standard deviation are often the best quantities to summarize the data for distributions with symmetrical histograms without too many outliers. As we can see from the histogram below, this indeed is the case for athlete heights. Therefore, the mean and the standard deviation measures are sufficient information and other tendencies such as the median does not add too much of extra information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz6CWo8ogAXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "heights = heights.fillna(heights.mean())\n",
        "sns.distplot(heights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYfWJM0vgAXh",
        "colab_type": "text"
      },
      "source": [
        "## The Normal Distribution\n",
        "\n",
        "The normal distribution is probably the most important and commonly occuring distribution in nature. The normal distribution was first arrived at by De Moivre when he was trying to come up with a continuous approximation to binomial distributions with p = 0.5. \n",
        "\n",
        "Let us now simulate a similar experiment as above. Let us sample a 1000 points from a normal distribution and plot the number of occurences in the form of a histogram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC-JzjkGgAXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outcomes = []\n",
        "for i in range(1000):\n",
        "    point = np.random.normal(0, 1)\n",
        "    outcomes.append(point)\n",
        "\n",
        "sns.distplot(outcomes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrYvLMgagAXr",
        "colab_type": "text"
      },
      "source": [
        "A major reason for the ubiquity of the Normal Distribution has to do with the **Central Limit Theorem**. Naively stated, data that are affected by small and independent effects tend to be normally distributed. Since this is true for most of the attributes found in nature, the normal distribution is ubiquitously observed.\n",
        "\n",
        "## Normal Tests\n",
        "\n",
        "We had stated earlier that heights and weights are approximately normally distributed. But how do we know this? To put it more generally, given a sample, how can we test if the distribution is normal?\n",
        "\n",
        "This is usually done in two ways:\n",
        "* **Histograms**: We have already covered this in a previous section. If the distribution shape is like a bell curve, we can be reasonably sure that it is normal.\n",
        "* **Normal Test**: The Scipy package gives us a very handy normaltest method that lets us calculate the probability that the distrbution is normal, by chance.\n",
        "\n",
        "Let us now check for the normalcy of the athlete weights in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX5IyhK9gAXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = df['weight']\n",
        "weights = weights.fillna(weights.mean())\n",
        "\n",
        "sns.distplot(weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOZHlawWgAX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stats.normaltest(weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdkK_UFlgAX9",
        "colab_type": "text"
      },
      "source": [
        "As can be seen above, the weights in our dataset are not normally distributed. The histrogram plot does not exactly resemble a bell curve and the normal test gives a p-value of 0, which means that there almost no chance that the distribution is normal.\n",
        "\n",
        "This observation of Olympic athlete weights is pretty interesting as human weights, in general, tend to be normally distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4EkTIdugAX_",
        "colab_type": "text"
      },
      "source": [
        "## Z-Score and P-Value\n",
        "\n",
        "The z-score and p-value are central to almost every statistical inference tool and hypothesis testing methods. The Z-score is measure of how many standard deviations away from the mean, a particular sample point is. The p-value gives us the probability that of getting a z-score less than or equal to the given z-score and in a sense, is a measure of the number of sample points that have a z-score less than or equal to the corresponding value of z.\n",
        "\n",
        "The Scipy package gives us two very useful functions to get the p-value for a given z-score and vice versa. The code below tries to calculate the number of sample points that fall within one, two and three standard deviations from the mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2pBR3k_gAYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pvalue(z):\n",
        "    return 1 - 2 * (1 - stats.norm.cdf(z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruWT9JvFgAYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pvalue(1), pvalue(2), pvalue(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC0lEuchgAYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zscore(frac):\n",
        "    return stats.norm.ppf(0.5 + frac/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNIYDTMRgAYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zscore(0.50), zscore(0.68), zscore(0.99)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QCXQyu9gAYg",
        "colab_type": "text"
      },
      "source": [
        "# Sampling\n",
        "\n",
        "In almost every field of study (natural sciences, politics, computer science), we often need statistics about our data. But more often than not, it is simply too impractical to gain all possible information to arrive at our summary and conclusions. For example, if we wanted to predict election results, it would be impractical to gather preference data from every eligible voter in the country. \n",
        "\n",
        "In such cases, we have to resort to cheaper methods that are more feasible. One such method is to gather data of a small fraction of the population that you believe represents the entire population well. The statistics gathered from this fraction of people would then be reflective of the population as a whole. This is called sampling.\n",
        "\n",
        "The Credit Card Fraud Detection dataset has been obtained from Kaggle courtesy Andrea and is available at https://www.kaggle.com/dalpozz/creditcardfraud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMDM9D1IgAYj",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/254/0*o0NliEWAfYeZ0WHZ.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjDzfHkrgAYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0csSkITgAYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "url='https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv'\n",
        "df = pd.read_csv(url,sep=\",\") # use sep=\",\" for coma separation. \n",
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIN2np-vgAYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEnfk3U7gAY2",
        "colab_type": "text"
      },
      "source": [
        "## Estimating a Population Proportion\n",
        "\n",
        "In the following sections, we are going to try and find the fraction of transactions that are fraudalent by examining data from only 5% of the sample. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7neRhW5gAY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_samp = df.sample(frac=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNQ5iYXwgAZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_samp['Class'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf6M48V7gAZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_hat = len(df_samp[df_samp['Class'] == 1]) / len(df_samp)\n",
        "p_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfXZgRWagAZL",
        "colab_type": "text"
      },
      "source": [
        "The value of p_hat obtained above is the fraction of fraudalent transactions in the random sample that Pandas gave us. But how is this p_hat value related to the real fraction p?\n",
        "\n",
        "Imagine that we didn't have the data for all the credit card transactions as we do now. How would we go about estimating the real fraction from the results of this small sample? As you may have guessed, it really isn't possible to determine the exact fraction with 100% accuracy. What we can do, though, is define a confidence interval and quantitatively state that we are this much confident that the real fraction is within a particular range. In doing so, we shift from a deterministic realm to the stochastic realm of samples governed by probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ7YIhPogAZM",
        "colab_type": "text"
      },
      "source": [
        "![alt text](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g4m_wuxgAZN",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/742/0*fAdTwLXc7S3X44CW.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJufvf7sgAZP",
        "colab_type": "text"
      },
      "source": [
        "In statistics, a confidence interval is a type of interval estimate, computed from the statistics of the observed data, that might contain the true value of an unknown population parameter.\n",
        "\n",
        "The p_hat that we obtained in a previous step is a random variable whose value will change in different trials of the experiment (sampling 5% of the population).\n",
        "Let's say that we conduct this experiment 1000 times. How will the p_hat obtained in each experiment be related to each other? Let's simulate the experiment, plot the distribution and find out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSdh8X9pgAZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_hat_samples = []\n",
        "for i in range(1000):\n",
        "    sample = df.sample(frac=0.05)\n",
        "    p_sample = len(sample[sample['Class'] == 1]) / len(sample)\n",
        "    p_hat_samples.append(p_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raRdVh_DgAZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(p_hat_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xu-ySkTPgAZd",
        "colab_type": "text"
      },
      "source": [
        "As can be seen above, the $\\hat{p}$'s are in the form of a normal distribution.  Without proof, we will present the following results:\n",
        "\n",
        "* E[$\\hat{p}$] = p.\n",
        "* $\\hat{\\sigma}$ = $\\sqrt{\\frac{p(1-p)}{n}}$\n",
        "* The distribution becomes normal with p as the mean as n approaches infinity.\n",
        "\n",
        "Therefore, the accuracy of our value is only dependent on the spread of our $\\hat{p}$. Since p is deterministic, we can say that it is dependent only on the sample size, n. The accuracy of our estimates is therefore determined by $\\frac{1}{\\sqrt{n}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9FY204-gAZf",
        "colab_type": "text"
      },
      "source": [
        "## Reporting our results\n",
        "\n",
        "The sample size determines the accuracy of our results. As mentioned earlier, we can never be 100% accurate with our results. We can only be confident to a certain level. From our previous notebook, we learnt about z-scores and p-values. We know that 68% of the values fall within one standard deviation.\n",
        "\n",
        "From this, we can directly say that the real value p falls within $\\hat{p}$ + $\\hat{\\sigma}$ and $\\hat{p}$ - $\\hat{\\sigma}$ and we can state this with 68% certainty.\n",
        "\n",
        "But what if we wanted a 99% certainty (or confidence level)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RJ7L450gAZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zscore(frac):\n",
        "    return stats.norm.ppf(0.5 + frac/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJIliBUGgAZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = zscore(0.99)\n",
        "z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SvwMG8bgAZt",
        "colab_type": "text"
      },
      "source": [
        "The above result tells us that 99% of the values fall within 2.575 standard deviations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du-SaZnjgAZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgDHzTKQgAZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sigma_hat = np.sqrt((p_hat * (1- p_hat))/len(df_samp))\n",
        "sigma_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhD7NZcCgAZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lower_limit = p_hat - z*sigma_hat\n",
        "upper_limit = p_hat + z*sigma_hat\n",
        "lower_limit, upper_limit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hmqo21QRgAaG",
        "colab_type": "text"
      },
      "source": [
        "From the above result, we can directly state that we are 99% confident that the real p lies within (0.00069716474925151251, 0.0023927228911979257).\n",
        "\n",
        "We can now present the following results:\n",
        "* There is a tradeoff between confidence level and range size. Higher the confidence, larger the range.\n",
        "* Increasing the sample size will lead to a reduced standard deviation and therefore, more accurate and practically significant results.\n",
        "\n",
        "Finally let us check if our results tally with what we've suggested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELcz9MU8gAaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = len(df[df['Class'] == 1])/len(df)\n",
        "p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyNZVmlqgAaP",
        "colab_type": "text"
      },
      "source": [
        "The value of p does indeed fall between the above range.\n",
        "\n",
        "Finally, let us check on the mean of the $\\hat{p}$'s we obtained from simulating the experiment a 1000 times. From our results, we know that this mean will approach p as n approaches infinity. Therefore, we should be reasonably confident that this mean is extremely close to the value of p."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMKd6dFagAaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "expected_p = pd.Series(p_hat_samples).mean()\n",
        "expected_p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLS2onRygAaW",
        "colab_type": "text"
      },
      "source": [
        "## Estimating the average weight of Women Olympians\n",
        "\n",
        "In the following sections, we will try and estimate the mean of the weights of female olympic athletes by taking a fraction of the data (artificially creating a sample). Estimation of the population mean is extremely similar to estimating the population proportion as is demonstrated below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlXeyZHBgAaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLeWDA6ygAae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = pd.read_csv('athletes.csv')\n",
        "df2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TvHr-hxgAaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = df2[df2['sex'] == 'female']\n",
        "df2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82K1DbJngAaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_hT-VtFgAa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2_samp = df2.sample(frac=0.1)\n",
        "df2_samp.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWSm-aPSgAa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_hat = df2_samp['weight'].mean()\n",
        "mean_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI-zraNagAbA",
        "colab_type": "text"
      },
      "source": [
        "Unlike the previous case, we are not dealing fractions but means. How will the $\\hat{\\mu}$'s be distributed? Turns out, they will be normally distributed as well! This famous result is known as the **Central Limit Theorem**. \n",
        "\n",
        "What is brilliant about this result is its complete lack of regard for the distribution of the original data. It doesn't matter how the data is related to one another; their sampled means will always be normally distributed. Like in the proportions case, let us simulate an experiment to visualise this result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqMqztL0gAbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_hat_samples = []\n",
        "for i in range(1000):\n",
        "    sample = df2.sample(frac=0.1)\n",
        "    mean_sample = sample['weight'].mean()\n",
        "    mean_hat_samples.append(mean_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFuiFlKngAbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(mean_hat_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3BcuDpEgAbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stats.normaltest(mean_hat_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0FucXi6gAbV",
        "colab_type": "text"
      },
      "source": [
        "The sampled means are indeed normally distributed as can be seen above. Let us now continue with our analysis of estimating the population mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjHL45JwgAbW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "std_hat = df2_samp['weight'].std()/np.sqrt(len(df2_samp))\n",
        "std_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugnRia6VgAbb",
        "colab_type": "text"
      },
      "source": [
        "For a change, let us calculate the confidence interval at a 95% confidence level this time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VojsjSqgAbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = zscore(0.95)\n",
        "z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2fhaQ-ZgAbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lower_limit = mean_hat - z*std_hat\n",
        "upper_limit = mean_hat + z*std_hat\n",
        "\n",
        "lower_limit, upper_limit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T42oPOEZgAbl",
        "colab_type": "text"
      },
      "source": [
        "We can now report that we are 95% confident that the average weight of female Olympians is within the above range\n",
        "\n",
        "Let us check our result with the actual mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh-c-mWCgAbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2['weight'].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bTo6KvHgAbq",
        "colab_type": "text"
      },
      "source": [
        "62.6444 does indeed fall between the above range. A general point to be noted is that the greater the confidence level, the greater the probability that the real value is in the range offered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFG5D7wngAbq",
        "colab_type": "text"
      },
      "source": [
        "## The T-Statistic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDdrZpBqgAbt",
        "colab_type": "text"
      },
      "source": [
        "The Studentâ€™s t-distribution is similar to the normal distribution, except it is more spread out and wider in appearance, and has thicker tails. The differences between the t-distribution and the normal distribution are more exaggerated when there are fewer data points, and therefore fewer degrees of freedom.\n",
        "\n",
        "The t-statistic is the ideal score to use for sample sizes lesser than 30. For sample sizes greater than 30, t and z scores are essentially the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMj30sORgAbw",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/574/1*LlBltIwkHXx6CgncSB_Oiw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWD0tdu_gAbx",
        "colab_type": "text"
      },
      "source": [
        "## Central Limit Theorem Test on Non Normal Distributions\n",
        "\n",
        "Let us check the merit of the central limit theorem by testing it on a non normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abA6WLDcgAbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_normal = pd.Series(np.random.randint(1000, size=1000))\n",
        "sns.distplot(non_normal)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvua_NMEgAb3",
        "colab_type": "text"
      },
      "source": [
        "This is clearly not normal. Now, let us try to simulate our little experiment on this distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nCws1d6gAb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot([non_normal.sample(frac=0.1).mean()  for i in range(1000)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzmepKs3gAb6",
        "colab_type": "text"
      },
      "source": [
        "# Hypothesis Testing\n",
        "\n",
        "![alt text](https://miro.medium.com/max/910/1*4c72kKs77I7nJmGtYq3dkw.png)\n",
        "\n",
        "## One Sample Significance Tests\n",
        "\n",
        "\n",
        "\n",
        "The purpose of One Sample Significance Tests is to check if a sample of observations could have been generated by a process with a specific mean or proportion.\n",
        "\n",
        "Some questions that can be answered by one sample significance tests are:\n",
        "* Is there equal representation of men and women in a particular industry?\n",
        "* Is the normal human body temperature 98.6 F?\n",
        "\n",
        "We will try and apply this test to a few real world problems in this notebook.\n",
        "\n",
        "The Suicide dataset was obtained from Kaggle courtesy Rajanand Illangovan. You can download it here: https://www.kaggle.com/rajanand/suicides-in-india"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caOwTQT1gAb7",
        "colab_type": "text"
      },
      "source": [
        "### Analyzing Suicides in India by Gender\n",
        "\n",
        "Are men as likely to commit suicide as women?\n",
        "\n",
        "This is the question we will attempt at answering in this section. To answer this question, we will use suicide statistics shared by the National Crime Records Bureau (NCRB), Govt of India. To perform this analysis, we need to know the sex ratio in India. The Census 2011 report states that there are 940 females for every 1000 males in India.\n",
        "\n",
        "Let p denote the fraction of women in India."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ4xVl6-gAb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = 940/(940+1000)\n",
        "p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gowq_lW0gAb_",
        "colab_type": "text"
      },
      "source": [
        "If there is no correlation between gender and suicide, then the sex ratio of people committing suicides should closely reflect that of the general population. \n",
        "\n",
        "Let us now get our data into a Pandas dataframe for analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A0eegNrgAcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "url='https://raw.githubusercontent.com/SankBad/GraduateSpecialistRutgers/master/suicides.csv'\n",
        "df = pd.read_csv(url,sep=\",\") # use sep=\",\" for coma separation. \n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3BSmTCIgAcE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlPE46MAgAcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Gender'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzO4L1iEgAcK",
        "colab_type": "text"
      },
      "source": [
        "We can see that the number of female suicides is slightly lesser than the number of male suicides. There are also fewer females than males. How do we prove that females are as likely to commit suicide as males? This can be answered through hypothesis testing.|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JesgupgHgAcL",
        "colab_type": "text"
      },
      "source": [
        "#### Step 1: Formulate the hypothesis and decide on confidence level\n",
        "\n",
        "The null hypothesis, as stated in the slides, is the default state. Therefore, I will state my null and alternate hypothesis as follows.\n",
        "\n",
        "* **Null Hypothesis (H0)**: Men and women are equally likely to commit suicide.\n",
        "* **Alternate Hypothesis (H1)**: Men and women are not equally likely to commit suicide.\n",
        "\n",
        "If the null hypothesis is true, it would mean that the fraction of women committing suicide would be the same as the fraction of women in the general population. We now need to use a suitable statistica test to find out if this is indeed is the case.\n",
        "\n",
        "Our statistical test will generate a p-value which has to be compared to a significance level ($\\alpha$). If p is less than alpha, then it is extremely unlikely that the event must have occurred by chance and we would be reasonable in rejecting the null hypothesis. On the contrary, if the p-value is higher than $\\alpha$, we will not be in a position to reject the null hypothesis.\n",
        "\n",
        "Let us assume, $\\alpha$ = 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17cy1JCMgAcL",
        "colab_type": "text"
      },
      "source": [
        "#### Step 2: Decide on the Statsitical Test\n",
        "\n",
        "We will be using the One Sample Z-Test here. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twN0PjBXgAcN",
        "colab_type": "text"
      },
      "source": [
        "#### Step 3: Compute the p-value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TFQE2SOgAcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h0_prop = p\n",
        "h0_prop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Q_B0jhgAcZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h1_prop = df['Gender'].value_counts()['Female']/len(df)\n",
        "h1_prop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95xQaNtPgAcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sigma_prop = np.sqrt((h0_prop * (1 - h0_prop))/len(df))\n",
        "sigma_prop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XmFrRhfgAck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = (h1_prop - h0_prop)/sigma_prop\n",
        "z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIbJ_ndVgAcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pvalue(z):\n",
        "    return 2 * (1 - stats.norm.cdf(z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk2uQCMwgAcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_val = (1-stats.norm.cdf(z))*2\n",
        "p_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HAh73xKgAct",
        "colab_type": "text"
      },
      "source": [
        "The p value is so small that Python has effectively rounded it to zero.\n",
        "\n",
        "#### Step 4: Comparison and Decision\n",
        "\n",
        "The p value obtained is extremely strong evidence to suggest that it is much lower than our significance level $\\alpha$. We can thus safely disregard the null hypothesis and accept the alternate hypothesis (since it is the negation of the null hypothesis).\n",
        "\n",
        "**Men and women are not equally likely to commit suicide.**\n",
        "\n",
        "Note that this test says nothing about if men are more likely than women to commit suicide or vice versa. It just states that they are not equally likely. The reader is encouraged to form their own hypothesis tests to check these results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM4dgKxigAcw",
        "colab_type": "text"
      },
      "source": [
        "### Analyzing the average heights of NBA Players\n",
        "\n",
        "I was interested in knowing the average height of NBA playes. A quick Google search tells me that the average height of players between 1985-2006 was **6'7\"** or 200.66 cm. Is this still the case?\n",
        "\n",
        "To answer this question, we will be using the NBA Players Stats - 2014-2015 dataset on Kaggle courtesy DrGuillermo. The dataset can be downloaded here: https://www.kaggle.com/drgilermo/nba-players-stats-20142015"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLhKRUEcgAc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrsMr-fzgAc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = pd.read_csv('players_stats.csv')\n",
        "df2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAhu2vI8gAdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEa5pXjsgAdH",
        "colab_type": "text"
      },
      "source": [
        "#### Hypothesis Testing\n",
        "\n",
        "One Sample Significance Test for Mean is extremely similar to that for Proportion. We will go through almost an identical process.\n",
        "\n",
        "The hypotheses are defined as follows:\n",
        "* **Null Hypothesis**: The average height of an NBA player is 200.66 cm.\n",
        "* **Alternate Hypothesis**: The average height of an NBA player is not 200.66 cm.\n",
        "\n",
        "Significance Level, $\\alpha$ is at 0.05. Assuming Null Hypothesis to be true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5zw7UregAdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h0_mean = 200.66"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp5jbemxgAdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "h1_mean = df2['Height'].mean()\n",
        "h1_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M303oe1ngAdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sigma = df2['Height'].std()/np.sqrt(len(df2))\n",
        "sigma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVv84daJgAdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = (h1_mean - h0_mean)/sigma\n",
        "z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITKTRbX1gAdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p_val = (1 - stats.norm.cdf(abs(z))) * 2\n",
        "p_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nGYvSHPgAdY",
        "colab_type": "text"
      },
      "source": [
        "The p value obtained is much lesser than the significance level $\\alpha$. We therefore reject the null hypothesis and accept the alternate hypothesis (the negation). We can therefore arrive at the following conclusion from this analysis:\n",
        "\n",
        "**The average height of NBA Players is NOT 6'7\"**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0913fG1gAdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}